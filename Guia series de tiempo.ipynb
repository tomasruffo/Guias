{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Series de tiempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• ¿Qué va a suceder en el futuro?\n",
    "• ¿Cuándo va a ocurrir?\n",
    "• ¿En qué medida?, ¿cuánto?\n",
    "• ¿Qué probabilidad hay de que realmente ocurra así?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time dependent. So the basic assumption of a linear regression model that the observations are independent doesn’t hold in this case.\n",
    "Along with an increasing or decreasing trend, most TS have some form of seasonality trends, i.e. variations specific to a particular time frame. For example, if you see the sales of a woolen jacket over time, you will invariably find higher sales in winter seasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Flujos: El valor es el acumulado a lo largo del tiempo\n",
    "Se puede sumar a lo largo del tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stock:El valor corresponde a un instante del periodo\n",
    "No se puede sumar a lo largo del tiempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evolucion en el timepo :\n",
    "  * Variables estacionarias: Media y varianza constantes (es mas simple)\n",
    "  * Variables no estacionarias: Media y varianza no constantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afinidad a la media: si trazo una recta y no la media no coincide se dice que no hay afinidad a la media\n",
    "Covarinaza = covarianza positiva(uan baja y la otra sube), covarianza negativa(una sube una baja)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TENDENCIA  ( se mide en 32 trimestres  genralmente)\n",
    "* CICLO capta las fluctuaciones en un periodo(2 a 8 años) asociada a las ocilaciones de corto plazo( movimientos caracterizados por  resupuestas de los agentes economicos a perturbaciones exogenas)\n",
    "* ESTACIONALIDAD ( mensuales), media = 0 y periodisidad anual\n",
    "* COMPONENTE IRREGULAR se comporta como ruido blanco y es impredesible(Estocastico dentro de la Serie Temporal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TEST DE HIPOTESIS: \n",
    "    * ADITIVA \n",
    "    * MULTIPLICATIVA\n",
    "    * MIXTA\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "estimacion de los componentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MEJOR DESCRIPTOR = MEJOR PREDICTOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas data\n",
    "\n",
    "https://unipython.com/analisis-de-series-temporales-con-la-libreria-pandas/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " La función to_datetime() crea marcas de tiempo a partir de cadenas de caracteres en una amplia variedad de formatos de fecha/hora. A continuación se importará Pandas y se convertirá algunas fechas y horas en marcas de tiempo (timestamps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> import pandas as pnd\n",
    ">>> pnd.to_datetime('2019-07-18 1:25pm')\n",
    "Timestamp('2019-07-18 13:25:00')\n",
    ">>> pnd.to_datetime('9/8/1993')\n",
    "Timestamp('1993-09-08 00:00:00')\n",
    "# dia primero\n",
    ">>> pnd.to_datetime('9/8/1993', dayfirst=True)  \n",
    "\n",
    "\n",
    "# si aclaramos el formato con la funcion\"format\", aceleramos el proseso de \"to_datatime\"\n",
    ">>> pnd.to_datetime(['18/07/19','9/8/93','15/12/03'], format='%d/%m/%Y')\n",
    "DatetimeIndex(['2019-07-18', '1993-08-09', '2003-12-15'], dtype='datetime64[ns]', freq=None)\n",
    "\n",
    "\n",
    "#Para definir el índice que se desea, se utilizan los parámetros index_col y parse_dates de la función read_csv(). Esto suele ser un atajo útil.\n",
    "opsd_dia = pnd.read_csv('opsd_germany_daily.csv', index_col=0, parse_dates=True)\n",
    ">>> opsd_dia.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convertir al formato fecha\n",
    "data[\"date\"].apply(lambda x: datetime.datetime.strptime(x, \"%d.%m.%Y\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-96f1960726db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#extraer dias de la semana\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'day of week'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Datetime'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdayofweek\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Datetime'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "#extraer dias de la semana\n",
    "train['day of week']=train['Datetime'].dt.dayofweek \n",
    "temp = train['Datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#asignar 0 si es dia de semana o 1 si es fin de semana\n",
    "#monday = 0 \n",
    "def applyer(row):\n",
    "    if row.dayofweek == 5 or row.dayofweek == 6:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0 \n",
    "temp2 = train['Datetime'].apply(applyer) \n",
    "train['weekend']=temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Timestamp = pd.to_datetime(train.Datetime,format='%d-%m-%Y %H:%M') \n",
    "train.index = train.Timestamp \n",
    "# Hourly time series \n",
    "hourly = train.resample('H').mean() \n",
    "# Converting to daily mean \n",
    "daily = train.resample('D').mean() \n",
    "# Converting to weekly mean \n",
    "weekly = train.resample('W').mean() \n",
    "# Converting to monthly mean \n",
    "monthly = train.resample('M').mean()\n",
    "\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(4,1) \n",
    "hourly.Count.plot(figsize=(15,8), title= 'Hourly', fontsize=14, ax=axs[0]) daily.Count.plot(figsize=(15,8), title= 'Daily', fontsize=14, ax=axs[1]) weekly.Count.plot(figsize=(15,8), title= 'Weekly', fontsize=14, ax=axs[2]) monthly.Count.plot(figsize=(15,8), title= 'Monthly', fontsize=14, ax=axs[3]) \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#promedio agrupado por año,de la variable count\n",
    "train.groupby(\"year\")[\"Count\"].mean().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> opsd_dia = pnd.read_csv('opsd_germany_daily.csv', index_col=0, parse_dates=True)\n",
    ">>> opsd_dia.index\n",
    "DatetimeIndex(['2006-01-01', '2006-01-02', '2006-01-03', '2006-01-04',\n",
    "'2006-01-05', '2006-01-06', '2006-01-07', '2006-01-08',\n",
    "'2006-01-09', '2006-01-10',\n",
    "...\n",
    "'2017-12-22', '2017-12-23', '2017-12-24', '2017-12-25',\n",
    "'2017-12-26', '2017-12-27', '2017-12-28', '2017-12-29',\n",
    "'2017-12-30', '2017-12-31'],\n",
    "dtype='datetime64[ns]', name='Date', length=4383, freq=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar datos de un solo dia\n",
    ">>> opsd_dia.loc['2015-07-18']\n",
    "Consumption     1186.87\n",
    "Wind            150.581\n",
    "Solar           148.598\n",
    "Wind+Solar      299.179\n",
    "Año                2015\n",
    "Mes                   7\n",
    "Dia            Saturday\n",
    "Name: 2015-07-18 00:00:00, dtype: object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datos de un intervalo\n",
    ">>> opsd_dia.loc['2013-08-15':'2013-08-20']\n",
    "            Consumption     Wind    Solar  Wind+Solar   Año  Mes       Dia\n",
    "Date\n",
    "2013-08-15     1230.894   41.649  161.266     202.915  2013    8  Thursday\n",
    "2013-08-16     1235.737   67.690  177.432     245.122  2013    8    Friday\n",
    "2013-08-17     1064.704   50.787  150.753     201.540  2013    8  Saturday\n",
    "2013-08-18     1003.820  139.001  104.760     243.761  2013    8    Sunday\n",
    "2013-08-19     1279.894   65.016   71.198     136.214  2013    8    Monday\n",
    "2013-08-20     1262.606   43.807  112.443     156.250  2013    8   Tuesday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'opsd_dia' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-6b1466ac2c42>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# se puede filtrar por mes, ano\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mopsd_dia\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'2009-03'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mopsd_dia\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'2008'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'opsd_dia' is not defined"
     ]
    }
   ],
   "source": [
    "# se puede filtrar por mes, ano \n",
    ">>> opsd_dia.loc['2009-03']\n",
    ">>> opsd_dia.loc['2008']\n",
    "\n",
    "\n",
    "## agrupar por meses y sacar promedio\n",
    "meses =df.resample('M').mean()\n",
    "meses\n",
    "\n",
    "\n",
    "# visualizar medias mensuales de cada año\n",
    "plt.plot(meses['2017'].values)\n",
    "plt.plot(meses['2018'].values)\n",
    "\n",
    "\n",
    "\n",
    "# cmparar dos meses \n",
    "\n",
    "verano2017 = df['2017-06-01':'2017-09-01']\n",
    "plt.plot(verano2017.values)\n",
    "verano2018 = df['2018-06-01':'2018-09-01']\n",
    "plt.plot(verano2018.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as mplt \n",
    "import seaborn as sbn\n",
    "sbn.set(rc={'figure.figsize':(10, 5)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opsd_dia['Consumption'].plot(linewidth=0.1)\n",
    "\n",
    "\n",
    "col_graf = ['Consumption', 'Solar', 'Wind'] \n",
    "ejes = opsd_dia[col_graf].plot(marker='.', alpha=0.1, linestyle='None',figsize=(10,10),subplots=True)\n",
    "for eje in ejes:\n",
    "  eje.set_ylabel('Consumo Diario (GWh)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datos solo de un mes( por ida)\n",
    "\n",
    "eje = opsd_dia.loc['2016-01':'2016-02', 'Consumption'].plot(marker='o',linestyle='-')\n",
    "eje.set_ylabel('Consumo Diario (GWh)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pnd\n",
    "import matplotlib.pyplot as mplt\n",
    "import seaborn as sbn\n",
    "\n",
    "opsd_dia = pnd.read_csv('opsd_germany_daily.csv', index_col=0, parse_dates=True)\n",
    "\n",
    "sbn.set(rc={'figure.figsize':(10, 5)})\n",
    "\n",
    "# 4to Gráfico\n",
    "eje = opsd_dia.loc['2016-01':'2016-02', 'Consumption'].plot(marker='o',linestyle='-')\n",
    "eje.set_ylabel('Consumo Diario (GWh)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Estacionalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agrupar datos por mes\n",
    "fig, ejes = mplt.subplots(3, 1, figsize=(11, 10), sharex=True)\n",
    "for nombre, eje in zip(['Consumption','Solar','Wind'], ejes):\n",
    "    sbn.boxplot(data=opsd_dia,x='Mes',y=nombre,ax=eje)\n",
    "    eje.set_title(nombre)\n",
    "    if eje != ejes[-1]:\n",
    "        eje.set_xlabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agrupar por semana\n",
    "import pandas as pnd\n",
    "import matplotlib.pyplot as mplt\n",
    "import seaborn as sbn\n",
    "\n",
    "opsd_dia = pnd.read_csv('opsd_germany_daily.csv', index_col=0, parse_dates=True)\n",
    "opsd_dia['Dia'] = opsd_dia.index.weekday_name\n",
    "\n",
    "sbn.set(rc={'figure.figsize':(10, 5)})\n",
    "   \n",
    "# 2do Gráfico\n",
    "sbn.boxplot(data=opsd_dia, x='Dia', y='Consumption')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frecuencias 2015-09-15  '2015-09-20 con frecuaencia diaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frecuencia dese \n",
    ">>> pnd.date_range('2015-09-15','2015-09-20',freq='D')                                        \n",
    "DatetimeIndex(['2015-09-15', '2015-09-16', '2015-09-17', '2015-09-18',                                       \n",
    "'2015-09-19', '2015-09-20'], dtype='datetime64[ns]', freq='D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las frecuencias disponibles de pandas incluyen cada hora (‘H’), calendario diario (‘D’), diario de negocios (‘B’), semanal (‘W’), mensual (‘M’), trimestral (‘Q’), anual (‘A’), tambien como múltiplos de cualquiera de las frecuencias base, por ejemplo, ‘5D’ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> frec_consumo = muestra_consumo.asfreq('D')\n",
    ">>> frec_consumo['Consumption - Datos llenos'] = muestra_consumo.asfreq('D', method='ffill')\n",
    ">>> frec_consumo\n",
    "            Consumption  Consumption - Datos llenos\n",
    "2010-03-02     1572.645                    1572.645\n",
    "2010-03-03          NaN                    1572.645\n",
    "2010-03-04     1548.184                    1548.184\n",
    "2010-03-05          NaN                    1548.184\n",
    "2010-03-06     1310.888                    1310.888"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remuestreo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> columnas = ['Consumption','Wind','Solar','Wind+Solar']\n",
    ">>> media_opsd_semanal = opsd_dia[columnas].resample('W').mean()\n",
    ">>> media_opsd_semanal.head(3)\n",
    "            Consumption  Wind  Solar  Wind+Solar\n",
    "Date\n",
    "2006-01-01  1069.184000   NaN    NaN         NaN\n",
    "2006-01-08  1381.300143   NaN    NaN         NaN\n",
    "2006-01-15  1486.730286   NaN    NaN         NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se trazarán las series temporales diarias y semanales para la energía solar (Solar) juntas en un solo período de seis meses para compararlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inicio, final = '2015-06','2015-12'\n",
    "fig,eje = mplt.subplots()\n",
    "eje.plot(opsd_dia.loc[inicio:final,'Solar'],marker='.',linestyle='-',linewidth=0.5,label='Diario')\n",
    "eje.plot(media_opsd_semanal.loc[inicio:final,'Solar'],marker='o',markersize=5,label='Semanal')\n",
    "eje.set_ylabel('Producción Solar (GWh)')\n",
    "eje.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> opsd_mensual = opsd_dia[columnas].resample('M').sum(min_count=28)\n",
    ">>> opsd_mensual.head(4)\n",
    "            Consumption  Wind  Solar  Wind+Solar\n",
    "Date\n",
    "2006-01-31    45304.704   NaN    NaN         NaN\n",
    "2006-02-28    41078.993   NaN    NaN         NaN\n",
    "2006-03-31    43978.124   NaN    NaN         NaN\n",
    "2006-04-30    38251.767   NaN    NaN         NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede notar que los datos remuestreados mensualmente están etiquetados con el final de cada mes (el limite derecho del intervalo), mientras que los datos remuestreados semanalmente están etiquetados con el inicio de cada semana. De forma predeterminada, los datos remuestreados se etiquetan con el límite derecho para las frecuencias mensuales, trimestrales y anuales, y con el límite izquierdo para todas las demás frecuencias. Este comportamiento y otras opciones se pueden ajustar utilizando los parámetros enumerados en la documentación de resample()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Ahora se explorarán las series temporales mensuales trazando el consumo de electricidad como un diagrama de línea, y la producción de energía eólica y solar juntas como un gráfico de área apilada.\n",
    "\n",
    "fig,eje = mplt.subplots()\n",
    "eje.plot(opsd_mensual['Consumption'],color='black',label='Consumo')\n",
    "opsd_mensual[['Wind','Solar']].plot.area(ax=eje,linewidth=0)\n",
    "eje.legend()\n",
    "eje.set_ylabel('Total Mensual (GWh)')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    ">>> opsd_anual = opsd_dia[columnas].resample('A').sum(min_count=360)\n",
    ">>> opsd_anual = opsd_anual.set_index(opsd_anual.index.year)\n",
    ">>> opsd_anual.index.name='Año'\n",
    ">>> opsd_anual['Wind+Solar/Consumo'] = opsd_anual['Wind+Solar'] / opsd_anual['Consumption']\n",
    ">>> opsd_anual.tail(4)\n",
    "       Consumption        Wind      Solar  Wind+Solar  Wind+Solar/Consumo\n",
    "Año\n",
    "2014  504164.82100   51107.672  32498.307   83370.502            0.165364\n",
    "2015  505264.56300   77468.994  34907.138  112376.132            0.222410\n",
    "2016  505927.35400   77008.126  34562.824  111570.950            0.220528\n",
    "2017  504736.36939  102667.365  35882.643  138550.008            0.274500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ventanas Deslizantes\n",
    "\n",
    "Las operaciones de ventanas deslizantes son otra transformación importante para los datos de series temporales. De manera similar al submuestreo, las ventanas deslizantes dividen los datos en ventanas de tiempo y los datos de cada ventana se agregan con una función como mean(), median(), sum(), entre otras. Sin embargo, a diferencia del submuestreo, donde los intervalos de tiempo no se superponen y la salida se encuentra a una frecuencia más baja que la entrada, las ventanas deslizantes se superponen y “deslizan” a la misma frecuencia que los datos, por lo que las series temporales transformadas tienen la misma frecuencia que las series temporales originales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el método rolling() se calcula la media deslizante de 7 días de datos diarios. Usando el argumento center=True, se etiqueta cada ventana en su punto medio, por lo que las ventanas deslizantes serán:\n",
    "\n",
    "2006-01-01 a 2006-01-07 – etiquetado como 2006-01-04\n",
    "2006-01-02 a 2006-01-08 – etiquetado como 2006-01-05\n",
    "2006-01-03 a 2006-01-09 – etiquetado como 2006-01-06\n",
    "y así sucesivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> opsd_7d = opsd_dia[columnas].rolling(7, center=True).mean()\n",
    ">>> opsd_7d.head(5)\n",
    "            Consumption  Wind  Solar  Wind+Solar\n",
    "Date\n",
    "2006-01-01          NaN   NaN    NaN         NaN\n",
    "2006-01-02          NaN   NaN    NaN         NaN\n",
    "2006-01-03          NaN   NaN    NaN         NaN\n",
    "2006-01-04  1361.471429   NaN    NaN         NaN\n",
    "2006-01-05  1381.300143   NaN    NaN         NaN\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pnd\n",
    "import matplotlib.pyplot as mplt\n",
    "import seaborn as sbn\n",
    "\n",
    "sbn.set(rc={'figure.figsize':(10, 5)})\n",
    "\n",
    "opsd_dia = pnd.read_csv('opsd_germany_daily.csv', index_col=0, parse_dates=True)\n",
    "opsd_7d = opsd_dia[columnas].rolling(7, center=True).mean()\n",
    "\n",
    "inicio, final='2015-06','2015-12'\n",
    "fig, eje = mplt.subplots()\n",
    "eje.plot(opsd_dia.loc[inicio:final, 'Solar'],marker='.',linestyle='-', linewidth=0.5,label='Diario')\n",
    "eje.plot(media_opsd_semanal.loc[inicio:final,'Solar'],marker='o',linestyle='-', linewidth=0.5,label='Semanal')\n",
    "eje.plot(opsd_7d.loc[inicio:final,'Solar'],marker='.',linestyle='-', linewidth=0.5,label='Media Deslizante de 7 Dias')\n",
    "eje.set_ylabel('Producción Solar (GWh)')\n",
    "eje.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tendencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El argumento min_periods=360 toma en cuenta\n",
    "# algunos dias faltantes en las medidas de energía solar\n",
    "# y eólica\n",
    "opsd_365d = opsd_dia[columnas].rolling(window=365,center=True,min_periods=360).mean()\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as mplt\n",
    "import seaborn as sbn\n",
    "\n",
    "sbn.set(rc={'figure.figsize':(10, 5)})\n",
    "\n",
    "opsd_dia = pnd.read_csv('opsd_germany_daily.csv', index_col=0, parse_dates=True)\n",
    "opsd_365d = opsd_dia[columnas].rolling(window=365,center=True,min_periods=360).mean()\n",
    "opsd_7d = opsd_dia[columnas].rolling(7, center=True).mean()\n",
    "\n",
    "fig, eje = mplt.subplots()\n",
    "eje.plot(opsd_dia['Consumption'], marker='.', markersize=2, color='0.6',\n",
    "linestyle='None', label='Diario')\n",
    "eje.plot(opsd_7d['Consumption'], linewidth=2, label='Media deslizante semanal')\n",
    "eje.plot(opsd_365d['Consumption'], color='0.2', linewidth=3,\n",
    "label='Tendencia (Media deslizante anual)')\n",
    "eje.legend()\n",
    "eje.set_xlabel('Año')\n",
    "eje.set_ylabel('Consumo (GWh)')\n",
    "eje.set_title('Tendencias en el Consumo Electrico')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Modelo autorregresivo integrado de media móvil\n",
    "tiene tres parametros (p,d,q)\n",
    "* P: Habla de la parte autoregresiva del modelo\n",
    "* D: Es el grado de diferenciacion, veces que los datos han restado versiones anteriores\n",
    "* Q: Orden del modelo de la media movil\n",
    "El pronóstico ARIMA para una serie temporal estacionaria no es más que una ecuación lineal (como una regresión lineal).\n",
    "¿Qué es una serie temporal estacionaria?\n",
    "Hay tres criterios básicos para que una serie se clasifique como serie estacionaria:\n",
    "\n",
    "La media de las series de tiempo no debe ser una función del tiempo. Debería ser constante.\n",
    "La varianza de la serie temporal no debe ser una función del tiempo.\n",
    "La covarianza del término i-ésimo y el término (i + m) no debería ser una función del tiempo.\n",
    "\n",
    "\n",
    "* ¿Qué es una serie temporal estacionaria?\n",
    "Hay tres criterios básicos para que una serie se clasifique como serie estacionaria:\n",
    "\n",
    "La media de las series de tiempo no debe ser una función del tiempo. Debería ser constante.\n",
    "La varianza de la serie temporal no debe ser una función del tiempo.\n",
    "La covarianza del término i-ésimo y el término (i + m) no debería ser una función del tiempo.\n",
    "\n",
    "* ¿Por qué tenemos que hacer estacionarias las series temporales?\n",
    "Hacemos que la serie sea estacionaria para que las variables sean independientes. Las variables pueden ser dependientes de varias maneras, pero solo pueden ser independientes de una manera. Entonces, obtendremos más información cuando sean independientes. Por lo tanto, la serie temporal debe ser estacionaria.\n",
    "\n",
    "Si la serie temporal no es estacionaria, primero tenemos que hacerla estacionaria. Para hacerlo, necesitamos eliminar la tendencia y la estacionalidad de los datos. Para obtener más información sobre la estacionariedad, puede consultar este artículo:\n",
    "\n",
    "\n",
    "\n",
    "* Chequeo de estacionaridad\n",
    "We use Dickey Fuller test to check the stationarity of the series.\n",
    "The intuition behind this test is that it determines how strongly a time series is defined by a trend.\n",
    "The null hypothesis of the test is that time series is not stationary (has some time-dependent structure).\n",
    "The alternate hypothesis (rejecting the null hypothesis) is that the time series is stationary.\n",
    "\n",
    "\n",
    "We interpret this result using the Test Statistics and critical value. If the Test Statistics is smaller than critical value, it suggests we reject the null hypothesis (stationary), otherwise a greater Test Statistics suggests we accept the null hypothesis (non-stationary).\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller \n",
    "def test_stationarity(timeseries):\n",
    "        #Determing rolling statistics\n",
    "    rolmean = pd.rolling_mean(timeseries, window=24) # 24 hours on each day\n",
    "    rolstd = pd.rolling_std(timeseries, window=24)\n",
    "        #Plot rolling statistics:\n",
    "    orig = plt.plot(timeseries, color='blue',label='Original')\n",
    "    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean & Standard Deviation')\n",
    "    plt.show(block=False)\n",
    "        #Perform Dickey-Fuller test:\n",
    "    print ('Results of Dickey-Fuller Test:')\n",
    "    dftest = adfuller(timeseries, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print (dfoutput)\n",
    "\n",
    "from matplotlib.pylab import rcParams \n",
    "rcParams['figure.figsize'] = 20,10\n",
    "test_stationarity(train_original['Count'])\n",
    "\n",
    "\n",
    "#Train_log = np.log(Train['Count']) \n",
    "valid_log = np.log(Test['Count'])\n",
    "moving_avg = Train_log.rolling( 24).mean()\n",
    "plt.plot(Train_log) \n",
    "plt.plot(moving_avg, color = 'red') \n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# eliminar tendencia\n",
    "train_log_moving_avg_diff = Train_log - moving_avg\n",
    "\n",
    "#dejamos caer los valores nulos\n",
    "\n",
    "train_log_moving_avg_diff.dropna(inplace = True) \n",
    "test_stationarity(train_log_moving_avg_diff)\n",
    "\n",
    "# estabilizar media\n",
    "train_log_diff = Train_log - Train_log.shift(1) \n",
    "test_stationarity(train_log_diff.dropna())\n",
    "\n",
    "\n",
    "\n",
    "Ahora descompondremos las series temporales en tendencia y estacionalidad y obtendremos el residual, que es la variación aleatoria en la serie.\n",
    "\n",
    "Eliminar la estacionalidad\n",
    "Por estacionalidad, nos referimos a fluctuaciones periódicas. Existe un patrón estacional cuando una serie está influenciada por factores estacionales (por ejemplo, el trimestre del año, el mes o el día de la semana).\n",
    "La estacionalidad es siempre de un período fijo y conocido.\n",
    "Utilizaremos la descomposición estacional para descomponer las series de tiempo en tendencia, estacionalidad y residuos.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose \n",
    "decomposition = seasonal_decompose(pd.DataFrame(Train_log).Count.values, freq = 24) \n",
    "\n",
    "trend = decomposition.trend \n",
    "seasonal = decomposition.seasonal \n",
    "residual = decomposition.resid \n",
    "\n",
    "plt.subplot(411) \n",
    "plt.plot(Train_log, label='Original') \n",
    "plt.legend(loc='best') \n",
    "plt.subplot(412) \n",
    "plt.plot(trend, label='Trend') \n",
    "plt.legend(loc='best') \n",
    "plt.subplot(413) \n",
    "plt.plot(seasonal,label='Seasonality') \n",
    "plt.legend(loc='best') \n",
    "plt.subplot(414) \n",
    "plt.plot(residual, label='Residuals') \n",
    "plt.legend(loc='best') \n",
    "plt.tight_layout() \n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Verifiquemos la estacionariedad de los residuos.\n",
    "\n",
    "train_log_decompose = pd.DataFrame(residual) \n",
    "train_log_decompose['date'] = Train_log.index \n",
    "train_log_decompose.set_index('date', inplace = True) train_log_decompose.dropna(inplace=True) \n",
    "test_stationarity(train_log_decompose[0])\n",
    "\n",
    "Pronosticar las series de tiempo usando ARIMA\n",
    "En primer lugar, ajustaremos el modelo ARIMA en nuestra serie de tiempo para eso tenemos que encontrar los valores optimizados para los parámetros p, d, q.\n",
    "\n",
    "Para encontrar los valores optimizados de estos parámetros, utilizaremos el gráfico ACF (función de autocorrelación) y PACF (función de autocorrelación parcial).\n",
    "\n",
    "ACF es una medida de la correlación entre TimeSeries con una versión retrasada de sí mismo.\n",
    "\n",
    "PACF mide la correlación entre TimeSeries con una versión retrasada de sí mismo, pero después de eliminar las variaciones ya explicadas por las comparaciones intermedias.\n",
    "\n",
    "\n",
    "\n",
    "Parcela ACF y PACF\n",
    "plt.plot(lag_acf) \n",
    "plt.axhline(y=0,linestyle='--',color='gray') plt.axhline(y=-1.96/np.sqrt(len(train_log_diff.dropna())),linestyle='--',color='gray') plt.axhline(y=1.96/np.sqrt(len(train_log_diff.dropna())),linestyle='--',color='gray') plt.title('Autocorrelation Function') \n",
    "plt.show() \n",
    "plt.plot(lag_pacf) \n",
    "plt.axhline(y=0,linestyle='--',color='gray') plt.axhline(y=-1.96/np.sqrt(len(train_log_diff.dropna())),linestyle='--',color='gray') plt.axhline(y=1.96/np.sqrt(len(train_log_diff.dropna())),linestyle='--',color='gray') plt.title('Partial Autocorrelation Function') \n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "El valor p es el valor de retraso donde el gráfico PACF cruza el intervalo de confianza superior por primera vez. Se puede notar que en este caso p = 1.\n",
    "El valor q es el valor de retraso donde el gráfico ACF cruza el intervalo de confianza superior por primera vez. Se puede notar que en este caso q = 1.\n",
    "\n",
    "Ahora haremos el modelo ARIMA ya que tenemos los valores p, q. Haremos los modelos AR y MA por separado y luego los combinaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Modelo AR\n",
    "El modelo autorregresivo especifica que la variable de salida depende linealmente de sus propios valores anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "model = ARIMA(Train_log, order=(2, 1, 0))  # here the q value is zero since it is just the AR model \n",
    "results_AR = model.fit(disp=-1)  \n",
    "plt.plot(train_log_diff.dropna(), label='original') \n",
    "plt.plot(results_AR.fittedvalues, color='red', label='predictions') \n",
    "plt.legend(loc='best') \n",
    "plt.show()\n",
    "\n",
    "\n",
    "Vamos a trazar la curva de validación para el modelo AR.\n",
    "\n",
    "Tenemos que cambiar la escala del modelo a la escala original.\n",
    "\n",
    "El primer paso sería almacenar los resultados pronosticados como una serie separada y observarlo.\n",
    "\n",
    "\n",
    "AR_predict=results_AR.predict(start=\"2014-06-25\", end=\"2014-09-25\") AR_predict=AR_predict.cumsum().shift().fillna(0) AR_predict1=pd.Series(np.ones(valid.shape[0]) * np.log(valid['Count'])[0], index = valid.index) \n",
    "AR_predict1=AR_predict1.add(AR_predict,fill_value=0) \n",
    "AR_predict = np.exp(AR_predict1)\n",
    "plt.plot(valid['Count'], label = \"Valid\") \n",
    "plt.plot(AR_predict, color = 'red', label = \"Predict\") \n",
    "plt.legend(loc= 'best') \n",
    "plt.title('RMSE: %.4f'% (np.sqrt(np.dot(AR_predict, valid['Count']))/valid.shape[0])) plt.show()\n",
    "\n",
    "MA_predict=results_MA.predict(start=\"2014-06-25\", end=\"2014-09-25\") \n",
    "MA_predict=MA_predict.cumsum().shift().fillna(0) \n",
    "MA_predict1=pd.Series(np.ones(Test.shape[0]) * np.log(Test['Count'])[0], index = Test.index) \n",
    "MA_predict1=MA_predict1.add(MA_predict,fill_value=0) \n",
    "MA_predict = np.exp(MA_predict1)\n",
    "plt.plot(Test['Count'], label = \"Valid\") \n",
    "plt.plot(MA_predict, color = 'red', label = \"Predict\") \n",
    "plt.legend(loc= 'best') \n",
    "plt.title('RMSE: %.4f'% (np.sqrt(np.dot(MA_predict, Test['Count']))/Test.shape[0]))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Modelo SARIMAX en series de tiempo diarias\n",
    "El modelo SARIMAX tiene en cuenta la estacionalidad de la serie temporal. Entonces construiremos un modelo SARIMAX en la serie de tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-dc4b542afe9c>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-dc4b542afe9c>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    y_hat_avg['SARIMA'] = fit1.predict(start=\"2014-6-25\", end=\"2014-9-25\", dynamic=True) plt.figure(figsize=(16,8))\u001b[0m\n\u001b[1;37m                                                                                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "y_hat_avg = valid.copy() \n",
    "fit1 = sm.tsa.statespace.SARIMAX(Train.Count, order=(2, 1, 4),seasonal_order=(0,1,1,7)).fit() \n",
    "y_hat_avg['SARIMA'] = fit1.predict(start=\"2014-6-25\", end=\"2014-9-25\", dynamic=True) plt.figure(figsize=(16,8)) \n",
    "plt.plot( Train['Count'], label='Train') \n",
    "plt.plot(valid['Count'], label='Valid') \n",
    "plt.plot(y_hat_avg['SARIMA'], label='SARIMA') \n",
    "plt.legend(loc='best') \n",
    "plt.show()\n",
    "\n",
    "rms = sqrt(mean_squared_error(valid.Count, y_hat_avg.SARIMA)) \n",
    "print(rms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizar observerd, trend ,seasonal and residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm \n",
    "sm.tsa.seasonal_decompose(Train.Count).plot() \n",
    "result = sm.tsa.stattools.adfuller(train.Count) \n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
