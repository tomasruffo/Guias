{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contenido teorico avanzado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INTERVIEW TEORICAL QUESTIONS: https://www.springboard.com/blog/machine-learning-interview-questions/\n",
    "* Transformaciones de fourier: https://betterexplained.com/articles/an-interactive-guide-to-the-fourier-transform/\n",
    "* Likelihood vs probability : https://stats.stackexchange.com/questions/2641/what-is-the-difference-between-likelihood-and-probability#2647\n",
    "* Generative and Discriminative Models: https://stackoverflow.com/questions/879432/what-is-the-difference-between-a-generative-and-a-discriminative-algorithm\n",
    "Cómo podemos saber si nuestro modelo es bueno?\n",
    "Teóricamente:\n",
    "Campo interesante: teoría de la generalización\n",
    "Según las ideas de medir la simplicidad/complejidad del modelo\n",
    "Intuición: formalización del principio de la navaja de Ockham\n",
    "Cuanto menos complejo es un modelo, más probable es que un buen resultado empírico no se deba simplemente a las peculiaridades de nuestra muestra\n",
    "\n",
    "Cuanto menos complejo sea un modelo de AA, más probable será que un buen resultado empírico no se deba simplemente a las peculiaridades de la muestra.\n",
    "\n",
    "\n",
    "Sugerencia\n",
    "Los conjuntos de prueba y de validación se \"desgastan\" con el uso repetido. Esto significa que cuanto más usas los mismos datos para tomar decisiones sobre la configuración de hiperparámetros u otras mejoras del modelo, menos seguridad tendrás de que esos resultados puedan realizar generalizaciones sobre datos nuevos nunca antes vistos. Ten en cuenta que los conjuntos de validación suelen desgastarse más lentamente que los conjuntos de prueba.\n",
    "\n",
    "Si es posible, una buena idea es recopilar más datos para \"actualizar\" el conjunto de prueba y el de validación. Comenzar desde cero es un gran restablecimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las condiciones del AA\n",
    "Las tres suposiciones básicas siguientes guían la generalización:\n",
    "\n",
    "Los ejemplos se obtienen independiente e idénticamente (i.i.d) de manera aleatoria de la distribución. En otras palabras, los ejemplos no se influyen entre sí. (Una explicación alternativa: i.i.d. es una forma de hacer referencia a la aleatoriedad de las variables).\n",
    "La distribución es estacionaria, es decir, no cambia dentro del conjunto de datos.\n",
    "Los ejemplos se obtienen de particiones de la misma distribución.\n",
    "En la práctica, a veces infringimos estas suposiciones. Por ejemplo:\n",
    "\n",
    "Considera un modelo que elige los anuncios para mostrar. La suposición de i.i.d. se infringiría si, en parte, el modelo basara su elección en función de los anuncios que el usuario visualizó anteriormente.\n",
    "Considera un conjunto de datos que contenga la información de ventas minoristas de un año. Las compras de los usuarios cambian todas las temporadas, lo cual infringiría la estacionariedad.\n",
    "Cuando sabemos que se infringe alguna de las tres suposiciones básicas anteriores, debemos prestar mucha atención a las métricas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diferencias entre regularizaciones L1 y L2\n",
    "L2 y L1 penalizan las ponderaciones de diferente modo:\n",
    "\n",
    "L2 penaliza ponderación2.\n",
    "L1 penaliza |ponderación|.\n",
    "En consecuencia, L2 y L1 tienen diferentes derivadas:\n",
    "\n",
    "La derivada de L2 es 2 * ponderación.\n",
    "La derivada de L1 es k (una constante cuyo valor es independiente de la ponderación).\n",
    "\n",
    "Una potencia de regularización de 0.1 debería ser suficiente. Ten en cuenta que se debe lograr un equilibrio: la regularización más potente nos da modelos más pequeños, pero puede afectar la pérdida de clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SESGOS\n",
    "https://developers.google.com/machine-learning/crash-course/fairness/types-of-bias?authuser=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPLEMENTACION DE MACHINE LEARNING \n",
    "https://developers.google.com/machine-learning/guides/rules-of-ml?authuser=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA EXPLORATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()\n",
    "data.tail()\n",
    "\n",
    "data.shape\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### loc and iloc are row-first, column-second. .[:10] loc inlcuye el 1o y iloc es hasta 9\n",
    "\n",
    "# loc obtiene filas (o columnas) con particular labels del índice.\n",
    "# iloc obtiene filas (o columnas) en determinadas posiciones en el índice (por lo que solo toma números enteros).\n",
    "# \n",
    "cols = ['country', 'variety']\n",
    "df = reviews.loc[:99, cols]\n",
    "or\n",
    "\n",
    "cols_idx = [0, 11]\n",
    "df = reviews.iloc[:100, cols_idx]\n",
    "\n",
    "#valores unicos\n",
    "reviews.taster_name.unique()\n",
    "\n",
    "\n",
    "#descripcion\n",
    "reviews.points.describe()\n",
    "\n",
    "#contador de valores\n",
    "\n",
    "reviews.taster_name.value_counts()\n",
    "\n",
    "#subdataset\n",
    "data = full_data.loc[:, [\"CLASS\", \"COUNTY\", \"geometry\"]].copy()\n",
    "\n",
    "\n",
    "# maps \n",
    "\n",
    "review_points_mean = reviews.points.mean()\n",
    "reviews.points.map(lambda p: p - review_points_mean)\n",
    "\n",
    "#contar variables\n",
    "reviews_per_country = reviews.country.value_counts()\n",
    "\n",
    "\n",
    "def remean_points(row):\n",
    "    row.points = row.points - review_points_mean\n",
    "    return row\n",
    "\n",
    "reviews.apply(remean_points, axis='columns')\n",
    "\n",
    "\n",
    "\n",
    "#I'm an economical wine buyer. Which wine is the \"best bargain\"? Create a variable bargain_wine with the title of the wine with the highest points-to-price ratio in the dataset\n",
    "bargain_idx = (reviews.points / reviews.price).idxmax()\n",
    "bargain_wine = reviews.loc[bargain_idx, 'title']\n",
    "\n",
    "\n",
    "#CONTADOR DE PALBRAS EN DSECRPCION\n",
    "\n",
    "n_trop = reviews.description.map(lambda desc: \"tropical\" in desc).sum()\n",
    "n_fruity = reviews.description.map(lambda desc: \"fruity\" in desc).sum()\n",
    "descriptor_counts = pd.Series([n_trop, n_fruity], index=['tropical', 'fruity'])\n",
    "\n",
    "# aplicar rattings de vino seguno su puntuacion(canada tiene 5 por pagar)\n",
    "\n",
    "def stars(row):\n",
    "    if row.country == 'Canada':\n",
    "        return 3\n",
    "    elif row.points >= 95:\n",
    "        return 3\n",
    "    elif row.points >= 85:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "star_ratings = reviews.apply(stars, axis='columns')\n",
    "\n",
    "\n",
    "# obtener mas barato para cada categoria \n",
    "reviews.groupby('points').price.min()\n",
    "\n",
    "#mejor obejto segun dos categorias\n",
    "\n",
    "reviews.groupby(['country', 'province']).apply(lambda df: df.loc[df.points.idxmax()])\n",
    "\n",
    "\n",
    "#analisis estadistico simple de nuestro dataset\n",
    "reviews.groupby(['country']).price.agg([len, min, max])\n",
    "\n",
    "#Ordenamiento de datos\n",
    "countries_reviewed.sort_values(by='len', ascending=False)\n",
    "\n",
    "# Ordenar con dos variables\n",
    "\n",
    "countries_reviewed.sort_values(by=['country', 'len'])\n",
    "\n",
    "# que vino es mejor por una cierta cantidad de dinero\n",
    "best_rating_per_price = reviews.groupby('price')['points'].max().sort_index()\n",
    "# minimo y maximo por variedad\n",
    "price_extremes = reviews.groupby(\"variety\").price.agg([min,max])\n",
    "\n",
    "\n",
    "# Remplazar NaN  por un valor determinado\n",
    "reviews.region_2.fillna(\"Unknown\")\n",
    "\n",
    "# Remplazar un valor por otro\n",
    "reviews.taster_twitter_handle.replace(\"@kerinokeefe\", \"@kerino\")\n",
    "\n",
    "# Re indexar fila\n",
    "\n",
    "reindexed = reviews.rename_axis('wines', axis='rows')\n",
    "\n",
    "loc = por nombre\n",
    "iloc =  por indice\n",
    "\n",
    "\n",
    "\n",
    "# Sacar valor si:\n",
    "print('Old size: %d' % len(train_df))\n",
    "train_df = train_df[(train_df.abs_diff_longitude < 5.0) & (train_df.abs_diff_latitude < 5.0)]\n",
    "print('New size: %d' % len(train_df))\n",
    "\n",
    "# Conditional selection\n",
    "data.loc[data.country == 'Italy']\n",
    "# dos condiciones\n",
    "reviews.loc[(reviews.country == 'Italy') & (reviews.points >= 90)]\n",
    "# or\n",
    "reviews.loc[(reviews.country == 'Italy') | (reviews.points >= 90)]\n",
    "#is in\n",
    "reviews.loc[reviews.country.isin(['Italy', 'France'])]\n",
    "\n",
    "#asignar data\n",
    "reviews['index_backwards'] = range(len(reviews), 0, -1)\n",
    "reviews['index_backwards']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agrupar por edad\n",
    "# Convertir edades en categorias\n",
    "edades = [0, 5, 12, 18, 35, 60, 100]\n",
    "categorias = [\"0-4\", \"5-11\", \"11-17\", \"18-34\", \"35-59\", \"60-100\"]\n",
    "\n",
    "cat_edad = pd.cut(data[\"Edad\"],edades, labels= categorias) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CORRELACION DE VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_dataframe = training_examples.copy()\n",
    "correlation_dataframe[\"target\"] = training_targets[\"median_house_value\"]\n",
    "\n",
    "correlation_dataframe.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRAFICOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar dos clases\n",
    "cat_cols = [\"telecommuting\", \"has_company_logo\", \"has_questions\", \"employment_type\", \"required_experience\", \"required_education\",]\n",
    "# visualizating catagorical variable by target\n",
    "import matplotlib.gridspec as gridspec # to do the grid of plots\n",
    "grid = gridspec.GridSpec(3, 3, wspace=0.5, hspace=0.5) # The grid of chart\n",
    "plt.figure(figsize=(15,25)) # size of figure\n",
    "\n",
    "# loop to get column and the count of plots\n",
    "for n, col in enumerate(cat_df[cat_cols]): \n",
    "    ax = plt.subplot(grid[n]) # feeding the figure of grid\n",
    "    sns.countplot(x=col, data=cat_df, hue='fraudulent', palette='Set2') \n",
    "    ax.set_ylabel('Count', fontsize=12) # y axis label\n",
    "    ax.set_title(f'{col} Distribution by Target', fontsize=15) # title label\n",
    "    ax.set_xlabel(f'{col} values', fontsize=12) # x axis label\n",
    "    xlabels = ax.get_xticklabels() \n",
    "    ylabels = ax.get_yticklabels() \n",
    "    ax.set_xticklabels(xlabels,  fontsize=10)\n",
    "    ax.set_yticklabels(ylabels,  fontsize=10)\n",
    "    plt.legend(fontsize=8)\n",
    "    plt.xticks(rotation=90) \n",
    "    total = len(cat_df)\n",
    "    sizes=[] # Get highest values in y\n",
    "    for p in ax.patches: # loop to all objects\n",
    "        height = p.get_height()\n",
    "        sizes.append(height)\n",
    "        ax.text(p.get_x()+p.get_width()/2.,\n",
    "                height + 3,\n",
    "                '{:1.2f}%'.format(height/total*100),\n",
    "                ha=\"center\", fontsize=10) \n",
    "    ax.set_ylim(0, max(sizes) * 1.15) #set y limit based on highest heights\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)= plt.subplots(ncols=2, figsize=(17, 5), dpi=100)\n",
    "length=text_df[text_df[\"fraudulent\"]==1]['description'].str.len()\n",
    "ax1.hist(length,bins = 20,color='orangered')\n",
    "ax1.set_title('Fake Post')\n",
    "length=text_df[text_df[\"fraudulent\"]==0]['description'].str.len()\n",
    "ax2.hist(length, bins = 20)\n",
    "ax2.set_title('Real Post')\n",
    "fig.suptitle('Characters in description')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRAFICO DE LA VARIABLE PRINCIPAL\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "sns.distplot(train['SalePrice'] , fit=norm);\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(train['SalePrice'])\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "\n",
    "#Now plot the distribution\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('SalePrice distribution')\n",
    "\n",
    "#Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(train['SalePrice'], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA WRANGLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remplazar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['Columna'] == 2207,'Columna/'] = 2007\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Columnas bucketizadas(revisar)\n",
    "https://www.tensorflow.org/api_docs/python/tf/feature_column/bucketized_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantile_based_boundaries(feature_values, num_buckets):\n",
    "  boundaries = np.arange(1.0, num_buckets) / num_buckets\n",
    "  quantiles = feature_values.quantile(boundaries)\n",
    "  return [quantiles[q] for q in quantiles.keys()]\n",
    "\n",
    "# Divide households into 7 buckets.\n",
    "households = tf.feature_column.numeric_column(\"households\")\n",
    "bucketized_households = tf.feature_column.bucketized_column(\n",
    "  households, boundaries=get_quantile_based_boundaries(\n",
    "    california_housing_dataframe[\"households\"], 7))\n",
    "\n",
    "# Divide longitude into 10 buckets.\n",
    "longitude = tf.feature_column.numeric_column(\"longitude\")\n",
    "bucketized_longitude = tf.feature_column.bucketized_column(\n",
    "  longitude, boundaries=get_quantile_based_boundaries(\n",
    "    california_housing_dataframe[\"longitude\"], 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recomendations:\n",
    "* if the column have 60% of missing values, drop the column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Columna\"] = data['columna'].fillna(full_df[\"YearBuilt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# porcentaje de valores vacios por columna\n",
    "total = df_train.isnull().sum().sort_values(ascending=False)\n",
    "percent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### dropear columnas con mas del 65% de los valores como  NA\n",
    "data_filter = data.loc[:, data.isnull().mean() <= .65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### dropear filas con mas de dos variables como na\n",
    "\n",
    "data_filter = data.dropna(how=\"any\")\n",
    "data_filter = data.dropna(subset =[\"column1\",\"column2\"])  #una de las dos variables tiene valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill the NaN values of each of the numerical columns with the median of each one\n",
    "\n",
    "for column in numerical_cols:\n",
    "    trainingData2[column].fillna(trainingData2[column].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill the NaN values of each of the categorical columns with the mode of each one\n",
    "\n",
    "for column in categorical_cols:\n",
    "    trainingData2[column].fillna(trainingData2[column].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the Nan values with \"Unknow\"\n",
    "\n",
    "data[\"columna\"].fillna(value=\"Unknow\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cantidad de NaN por columna\n",
    "nullColumns = []    #Lista que contendrá las columnas con valores Nulos\n",
    "for column in data.columns:\n",
    "    if data[column].isnull().sum() > 0:\n",
    "        print(column, data[column].isnull().sum())\n",
    "        nullColumns.append(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical cols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Low and High cardinality split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = [cname for cname in pfeatures.columns if \n",
    "                pfeatures[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "categorical_cols = [cname for cname in pfeatures.columns if \n",
    "                   pfeatures[cname].dtype == \"object\"]\n",
    "\n",
    "\n",
    "\n",
    "#prueba de la cardinalidad\n",
    "#A las columnas con baja cardinalidad,se le aplicara un One-Hot-Encoder\n",
    "low_cardinality_cols = [col for col in categorical_cols if data[col].nunique() < 10]\n",
    "\n",
    "#A las columnas con alta cardinalidad, se le aplicara un label_encoder\n",
    "high_cardinality_cols = list(set(categorical_cols)-set(low_cardinality_cols))\n",
    "\n",
    "low_cardinality_cols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols = pd.DataFrame(OH_encoder.fit_transform(data[low_cardinality_cols]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columnas de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASETS IMBALANCEADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARAR VARIABLE OBJETIVO CON LA OTRA\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(17, 5), dpi=100)\n",
    "plt.tight_layout()\n",
    "\n",
    "df[\"fraudulent\"].value_counts().plot(kind='pie', ax=axes[0], labels=['Real Post (95%)', 'Fake Post (5%)'])\n",
    "temp = df[\"fraudulent\"].value_counts()\n",
    "sns.barplot(temp.index, temp, ax=axes[1])\n",
    "\n",
    "axes[0].set_ylabel(' ')\n",
    "axes[1].set_ylabel(' ')\n",
    "axes[1].set_xticklabels([\"Real Post (17014) [0's]\", \"Fake Post (866) [1's]\"])\n",
    "\n",
    "axes[0].set_title('Target Distribution in Dataset', fontsize=13)\n",
    "axes[1].set_title('Target Count in Dataset', fontsize=13)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIMENSION REDUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESTANDARIZAR DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estadarizo los datos para que funcione mejor el modelo\n",
    "from  sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc_X = StandardScaler()\n",
    "X_train_sca = sc_X.fit_transform(X_train)\n",
    "X_test_sca = sc_X.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA SETS DESBALANCEADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smote\n",
    "#import the libaray for SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Separate input features and target\n",
    "y = data[\"Class\"]\n",
    "X = data.drop('Class', axis=1)\n",
    "\n",
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "sm = SMOTE(random_state=27,sampling_strategy=1.0)\n",
    "X_train, y_train = sm.fit_sample(X_train, y_train)\n",
    "\n",
    "smote_logistic = LogisticRegression()\n",
    "smote_logistic.fit(X_train,y_train)\n",
    "\n",
    "smote_pred = smote_logistic.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL APPLICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LINEAR REGRESSION WITH TENSORFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input feature: total_rooms.\n",
    "my_feature = california_housing_dataframe[[\"total_rooms\"]]\n",
    "# Define the label.\n",
    "targets = california_housing_dataframe[\"median_house_value\"]\n",
    "\n",
    "# Configure a numeric feature column for total_rooms.\n",
    "feature_columns = [tf.feature_column.numeric_column(\"total_rooms\")]\n",
    "# Use gradient descent as the optimizer for training the model.\n",
    "my_optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.0000001)\n",
    "my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
    "\n",
    "# Configure the linear regression model with our feature columns and optimizer.\n",
    "# Set a learning rate of 0.0000001 for Gradient Descent.\n",
    "linear_regressor = tf.estimator.LinearRegressor(\n",
    "    feature_columns=feature_columns,\n",
    "    optimizer=my_optimizer\n",
    ")\n",
    "\n",
    "def my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):\n",
    "    \"\"\"Trains a linear regression model of one feature.\n",
    "  \n",
    "    Args:\n",
    "      features: pandas DataFrame of features\n",
    "      targets: pandas DataFrame of targets\n",
    "      batch_size: Size of batches to be passed to the model\n",
    "      shuffle: True or False. Whether to shuffle the data.\n",
    "      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely\n",
    "    Returns:\n",
    "      Tuple of (features, labels) for next data batch\n",
    "    \"\"\"\n",
    "  \n",
    "    # Convert pandas data into a dict of np arrays.\n",
    "    features = {key:np.array(value) for key,value in dict(features).items()}                                           \n",
    " \n",
    "    # Construct a dataset, and configure batching/repeating.\n",
    "    ds = Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit\n",
    "    ds = ds.batch(batch_size).repeat(num_epochs)\n",
    "    \n",
    "    # Shuffle the data, if specified.\n",
    "    if shuffle:\n",
    "      ds = ds.shuffle(buffer_size=10000)\n",
    "    \n",
    "    # Return the next batch of data.\n",
    "    features, labels = ds.make_one_shot_iterator().get_next()\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "_ = linear_regressor.train(\n",
    "    input_fn = lambda:my_input_fn(my_feature, targets),\n",
    "    steps=100\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Create an input function for predictions.\n",
    "# Note: Since we're making just one prediction for each example, we don't \n",
    "# need to repeat or shuffle the data here.\n",
    "prediction_input_fn =lambda: my_input_fn(my_feature, targets, num_epochs=1, shuffle=False)\n",
    "\n",
    "# Call predict() on the linear_regressor to make predictions.\n",
    "predictions = linear_regressor.predict(input_fn=prediction_input_fn)\n",
    "\n",
    "# Format predictions as a NumPy array, so we can calculate error metrics.\n",
    "predictions = np.array([item['predictions'][0] for item in predictions])\n",
    "\n",
    "# Print Mean Squared Error and Root Mean Squared Error.\n",
    "mean_squared_error = metrics.mean_squared_error(predictions, targets)\n",
    "root_mean_squared_error = math.sqrt(mean_squared_error)\n",
    "print(\"Mean Squared Error (on training data): %0.3f\" % mean_squared_error)\n",
    "print(\"Root Mean Squared Error (on training data): %0.3f\" % root_mean_squared_error)\n",
    "\n",
    "\n",
    "\n",
    "min_house_value = california_housing_dataframe[\"median_house_value\"].min()\n",
    "max_house_value = california_housing_dataframe[\"median_house_value\"].max()\n",
    "min_max_difference = max_house_value - min_house_value\n",
    "\n",
    "print(\"Min. Median House Value: %0.3f\" % min_house_value)\n",
    "print(\"Max. Median House Value: %0.3f\" % max_house_value)\n",
    "print(\"Difference between Min. and Max.: %0.3f\" % min_max_difference)\n",
    "print(\"Root Mean Squared Error: %0.3f\" % root_mean_squared_error)\n",
    "\n",
    "\n",
    "sample = california_housing_dataframe.sample(n=300)\n",
    "\n",
    "\n",
    "# Get the min and max total_rooms values.\n",
    "x_0 = sample[\"total_rooms\"].min()\n",
    "x_1 = sample[\"total_rooms\"].max()\n",
    "\n",
    "# Retrieve the final weight and bias generated during training.\n",
    "weight = linear_regressor.get_variable_value('linear/linear_model/total_rooms/weights')[0]\n",
    "bias = linear_regressor.get_variable_value('linear/linear_model/bias_weights')\n",
    "\n",
    "# Get the predicted median_house_values for the min and max total_rooms values.\n",
    "y_0 = weight * x_0 + bias \n",
    "y_1 = weight * x_1 + bias\n",
    "\n",
    "# Plot our regression line from (x_0, y_0) to (x_1, y_1).\n",
    "plt.plot([x_0, x_1], [y_0, y_1], c='r')\n",
    "\n",
    "# Label the graph axes.\n",
    "plt.ylabel(\"median_house_value\")\n",
    "plt.xlabel(\"total_rooms\")\n",
    "\n",
    "# Plot a scatter plot from our data sample.\n",
    "plt.scatter(sample[\"total_rooms\"], sample[\"median_house_value\"])\n",
    "\n",
    "# Display graph.\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def train_model(learning_rate, steps, batch_size, input_feature=\"total_rooms\"):\n",
    "  \"\"\"Trains a linear regression model of one feature.\n",
    "  \n",
    "  Args:\n",
    "    learning_rate: A `float`, the learning rate.\n",
    "    steps: A non-zero `int`, the total number of training steps. A training step\n",
    "      consists of a forward and backward pass using a single batch.\n",
    "    batch_size: A non-zero `int`, the batch size.\n",
    "    input_feature: A `string` specifying a column from `california_housing_dataframe`\n",
    "      to use as input feature.\n",
    "  \"\"\"\n",
    "  \n",
    "  periods = 10\n",
    "  steps_per_period = steps / periods\n",
    "\n",
    "  my_feature = input_feature\n",
    "  my_feature_data = california_housing_dataframe[[my_feature]]\n",
    "  my_label = \"median_house_value\"\n",
    "  targets = california_housing_dataframe[my_label]\n",
    "\n",
    "  # Create feature columns.\n",
    "  feature_columns = [tf.feature_column.numeric_column(my_feature)]\n",
    "  \n",
    "  # Create input functions.\n",
    "  training_input_fn = lambda:my_input_fn(my_feature_data, targets, batch_size=batch_size)\n",
    "  prediction_input_fn = lambda: my_input_fn(my_feature_data, targets, num_epochs=1, shuffle=False)\n",
    "  \n",
    "  # Create a linear regressor object.\n",
    "  my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
    "  linear_regressor = tf.estimator.LinearRegressor(\n",
    "      feature_columns=feature_columns,\n",
    "      optimizer=my_optimizer\n",
    "  )\n",
    "\n",
    "  # Set up to plot the state of our model's line each period.\n",
    "  plt.figure(figsize=(15, 6))\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plt.title(\"Learned Line by Period\")\n",
    "  plt.ylabel(my_label)\n",
    "  plt.xlabel(my_feature)\n",
    "  sample = california_housing_dataframe.sample(n=300)\n",
    "  plt.scatter(sample[my_feature], sample[my_label])\n",
    "  colors = [cm.coolwarm(x) for x in np.linspace(-1, 1, periods)]\n",
    "\n",
    "  # Train the model, but do so inside a loop so that we can periodically assess\n",
    "  # loss metrics.\n",
    "  print(\"Training model...\")\n",
    "  print(\"RMSE (on training data):\")\n",
    "  root_mean_squared_errors = []\n",
    "  for period in range (0, periods):\n",
    "    # Train the model, starting from the prior state.\n",
    "    linear_regressor.train(\n",
    "        input_fn=training_input_fn,\n",
    "        steps=steps_per_period\n",
    "    )\n",
    "    # Take a break and compute predictions.\n",
    "    predictions = linear_regressor.predict(input_fn=prediction_input_fn)\n",
    "    predictions = np.array([item['predictions'][0] for item in predictions])\n",
    "    \n",
    "    # Compute loss.\n",
    "    root_mean_squared_error = math.sqrt(\n",
    "        metrics.mean_squared_error(predictions, targets))\n",
    "    # Occasionally print the current loss.\n",
    "    print(\"  period %02d : %0.2f\" % (period, root_mean_squared_error))\n",
    "    # Add the loss metrics from this period to our list.\n",
    "    root_mean_squared_errors.append(root_mean_squared_error)\n",
    "    # Finally, track the weights and biases over time.\n",
    "    # Apply some math to ensure that the data and line are plotted neatly.\n",
    "    y_extents = np.array([0, sample[my_label].max()])\n",
    "    \n",
    "    weight = linear_regressor.get_variable_value('linear/linear_model/%s/weights' % input_feature)[0]\n",
    "    bias = linear_regressor.get_variable_value('linear/linear_model/bias_weights')\n",
    "\n",
    "    x_extents = (y_extents - bias) / weight\n",
    "    x_extents = np.maximum(np.minimum(x_extents,\n",
    "                                      sample[my_feature].max()),\n",
    "                           sample[my_feature].min())\n",
    "    y_extents = weight * x_extents + bias\n",
    "    plt.plot(x_extents, y_extents, color=colors[period]) \n",
    "  print(\"Model training finished.\")\n",
    "\n",
    "  # Output a graph of loss metrics over periods.\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plt.ylabel('RMSE')\n",
    "  plt.xlabel('Periods')\n",
    "  plt.title(\"Root Mean Squared Error vs. Periods\")\n",
    "  plt.tight_layout()\n",
    "  plt.plot(root_mean_squared_errors)\n",
    "\n",
    "  # Output a table with calibration data.\n",
    "  calibration_data = pd.DataFrame()\n",
    "  calibration_data[\"predictions\"] = pd.Series(predictions)\n",
    "  calibration_data[\"targets\"] = pd.Series(targets)\n",
    "  display.display(calibration_data.describe())\n",
    "\n",
    "  print(\"Final RMSE (on training data): %0.2f\" % root_mean_squared_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "import xgboost as xgb\n",
    "model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.05, max_depth=3, \n",
    "                             min_child_weight=1.7817, n_estimators=2200,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213, silent=1,\n",
    "                             random_state =7, nthread = -1)\n",
    "model_xgb.fit(X_train, y_train)\n",
    "r2_score(model_xgb.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc2 = XGBClassifier(n_estimators=1000, learning_rate=0.40)\n",
    "\n",
    "\n",
    "xgbc2.fit(X_train, y_train,\n",
    "          early_stopping_rounds  = 10 ,\n",
    "          eval_set=[(X_test, y_test)], \n",
    "             verbose=False\n",
    "         )\n",
    "\n",
    "\n",
    "xgbc2_pred =xgbc2.predict(X_test)\n",
    "print(classification_report(y_test,xgbc2_pred))\n",
    "confusion_matrix(y_test, xgbc2_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_steps_with_tensor_flow.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEAN SQUARE ERROR AND ROOT MEAN\n",
    " Print Mean Squared Error and Root Mean Squared Error.\n",
    "mean_squared_error = metrics.mean_squared_error(predictions, targets)\n",
    "root_mean_squared_error = math.sqrt(mean_squared_error)\n",
    "print(\"Mean Squared Error (on training data): %0.3f\" % mean_squared_error)\n",
    "print(\"Root Mean Squared Error (on training data): %0.3f\" % root_mean_squared_error)\n",
    "\n",
    "\n",
    "min_house_value = california_housing_dataframe[\"median_house_value\"].min()\n",
    "max_house_value = california_housing_dataframe[\"median_house_value\"].max()\n",
    "min_max_difference = max_house_value - min_house_value\n",
    "\n",
    "print(\"Min. Median House Value: %0.3f\" % min_house_value)\n",
    "print(\"Max. Median House Value: %0.3f\" % max_house_value)\n",
    "print(\"Difference between Min. and Max.: %0.3f\" % min_max_difference)\n",
    "print(\"Root Mean Squared Error: %0.3f\" % root_mean_squared_error)\n",
    "\n",
    "\n",
    "\n",
    "calibration_data = pd.DataFrame()\n",
    "calibration_data[\"predictions\"] = pd.Series(predictions)\n",
    "calibration_data[\"targets\"] = pd.Series(targets)\n",
    "calibration_data.describe()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEORIA HIPERPARAMETROS\n",
    " ### ¿Hay una heurística estándar para el ajuste del modelo?\n",
    "\n",
    "Esta es una pregunta frecuente. La respuesta breve es que los efectos de los diferentes hiperparámetros dependen de los datos. Por lo tanto, no hay reglas estrictas; debes probarlos en tus datos.\n",
    "\n",
    "Dicho esto, aquí se incluyen algunas reglas generales que pueden ayudarte como guía:\n",
    "\n",
    " * El error de entrenamiento debe disminuir constantemente, de manera abrupta al principio y eventualmente estancarse a medida que converge el entrenamiento.\n",
    " * Si el entrenamiento no convirgió, prueba ejecutarlo durante más tiempo.\n",
    " * Si el error de entrenamiento disminuye muy lentamente, aumentar la tasa de entrenamiento puede ayudar a que disminuya más rápido.\n",
    "   * Sin embargo, en algunas ocasiones puede ocurrir exactamente lo opuesto si la tasa de aprendizaje es demasiado alta.\n",
    " * Si el error de entrenamiento varía extremadamente, prueba disminuir la tasa de aprendizaje.\n",
    "   * Una tasa de aprendizaje más baja con un número más alto de pasos o un tamaño del lote más grande suelen ser una buena combinación.\n",
    " * Los tamaños del lote muy pequeños también pueden causar inestabilidad. Primero prueba valores más altos, como 100 o 1,000, y disminúyelos hasta que observes degradación.\n",
    "\n",
    "Como dijimos antes, nunca te rijas estrictamente por estas reglas generales, porque los efectos dependen de los datos. Siempre debes experimentar y verificar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
